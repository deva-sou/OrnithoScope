{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import copy\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/acarlier/code/OrnithoScope/saved_models/model_efficientDet1.tflite'\n",
    "input_csv_path = '/home/acarlier/code/OrnithoScope/data/input.csv'\n",
    "columns_names = [\"split_value\", \"file_path\", \"label\",\n",
    "                      \"x_min\", \"y_min\", \"empty_1\", \"empty_2\",\n",
    "                      \"x_max\", \"y_max\", \"empty_3\"]\n",
    "output_images_directory = '/home/acarlier/code/data_ornitho/output_images/test_pickle/'\n",
    "comparaison_images_directory = '/home/acarlier/code/data_ornitho/comparaison_verite_terrain/comparaison_efficientDet1/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the trained TFLite model and define some visualization functions\n",
    "\n",
    "def get_classes_from_input():\n",
    "    input = pd.read_csv(input_csv_path, names = columns_names)\n",
    "    return input[\"label\"].unique().tolist()\n",
    "\n",
    "# Load the labels into a list\n",
    "classes = get_classes_from_input()\n",
    "# Define a list of colors for visualization\n",
    "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "  original_image = img\n",
    "  resized_img = tf.image.resize(img, input_size)\n",
    "  resized_img = resized_img[tf.newaxis, :]\n",
    "  resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
    "  #print(tf.shape(resized_img))\n",
    "  return resized_img, original_image\n",
    "\n",
    "def preprocess_images(image_paths, input_size):\n",
    "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "  img = tf.io.read_file(image_paths[0])\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "  resized_img = tf.image.resize(img, input_size)\n",
    "  resized_img = resized_img[tf.newaxis, :]\n",
    "  resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
    "  \n",
    "  resized_imgs = resized_img\n",
    "  \n",
    "  for i in range(1,len(image_paths)):\n",
    "    img = tf.io.read_file(image_paths[i])\n",
    "    img = tf.io.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "    resized_img = tf.image.resize(img, input_size)\n",
    "    resized_img = resized_img[tf.newaxis, :]\n",
    "    resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
    "    \n",
    "    resized_imgs = tf.concat([resized_imgs, resized_img], axis=0)\n",
    "    #print(tf.shape(resized_imgs))\n",
    "    \n",
    "  return resized_imgs\n",
    "\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "\n",
    "  signature_fn = interpreter.get_signature_runner()\n",
    "  #print(\"avant modèle\")\n",
    "  # Feed the input image to the model\n",
    "  output = signature_fn(images=image)\n",
    "  #interpreter.set_tensor(0, image)\n",
    "  #interpreter.invoke()\n",
    "  #print(\"après modèle\")\n",
    "  \n",
    "  # Get all outputs from the model\n",
    "  count = int(np.squeeze(output['output_0']))\n",
    "  scores = np.squeeze(output['output_1'])\n",
    "  classes = np.squeeze(output['output_2'])\n",
    "  boxes = np.squeeze(output['output_3'])\n",
    "\n",
    "  results = []\n",
    "  for i in range(count):\n",
    "    if scores[i] >= threshold:\n",
    "      result = {\n",
    "        'bounding_box': boxes[i],\n",
    "        'class_id': classes[i],\n",
    "        'score': scores[i]\n",
    "      }\n",
    "      results.append(result)\n",
    "  return results\n",
    "\n",
    "\n",
    "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
    "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
    "  # Load the input shape required by the model\n",
    "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
    "  #print(\"interpreter\")\n",
    "  \n",
    "  # Load the input image and preprocess it\n",
    "  preprocessed_image, _ = preprocess_image(\n",
    "      image_path,\n",
    "      (input_height, input_width)\n",
    "    )\n",
    "  #print(\"preprocessing\")\n",
    "  \n",
    "  # Run object detection on the input image\n",
    "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
    "  #print(\"detection\")\n",
    "  \n",
    "  if 0:\n",
    "    # Plot the detection results on the input image\n",
    "    original_image_np = original_image.numpy().astype(np.uint8)\n",
    "    for obj in results:\n",
    "      # Convert the object bounding box from relative coordinates to absolute\n",
    "      # coordinates based on the original image resolution\n",
    "      ymin, xmin, ymax, xmax = obj['bounding_box']\n",
    "      xmin = int(xmin * original_image_np.shape[1])\n",
    "      xmax = int(xmax * original_image_np.shape[1])\n",
    "      ymin = int(ymin * original_image_np.shape[0])\n",
    "      ymax = int(ymax * original_image_np.shape[0])\n",
    "\n",
    "      # Find the class index of the current object\n",
    "      class_id = int(obj['class_id'])\n",
    "\n",
    "      # Draw the bounding box and label on the image\n",
    "      color = [int(c) for c in COLORS[class_id]]\n",
    "      cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "      # Make adjustments to make the label visible for all objects\n",
    "      y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "      label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
    "      cv2.putText(original_image_np, label, (xmin, y),\n",
    "          cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Return the final image\n",
    "    original_uint8 = original_image_np.astype(np.uint8)\n",
    "  original_uint8 = None  \n",
    "  return original_uint8, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Génération des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090002_(13.0).jpg\n",
      "3516\n"
     ]
    }
   ],
   "source": [
    "def get_test_images():\n",
    "    input = pd.read_csv(input_csv_path, names = columns_names)\n",
    "    test_dataset = input.loc[input['split_value'] == 'TEST']\n",
    "    list_image_test = test_dataset[\"file_path\"].unique().tolist() # return all unique file_path to test, no prediction on two same image\n",
    "    return list_image_test\n",
    "loi = get_test_images()\n",
    "print(loi[0])\n",
    "print(len(loi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_test_dataset():\n",
    "    results = []\n",
    "    list_of_images = get_test_images()\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    #list_of_images = ['/home/acarlier/code/test/2021-06-11-13-44-43.jpg']\n",
    "    # Load the TFLite model\n",
    "    print(len(list_of_images),' to predict')\n",
    "    for img in list_of_images:\n",
    "        if list_of_images.index(img)%100 == 0:\n",
    "            print(list_of_images.index(img))\n",
    "        img_name = img.split('/')[-1]\n",
    "        file_path = '{}{}'.format(output_images_directory, img_name)\n",
    "        if os.path.exists(file_path):\n",
    "            list_of_images.remove(img)\n",
    "            #print(len(list_of_images),' remaining')\n",
    "        else:\n",
    "            INPUT_IMAGE_URL = img #@param {type:\"string\"}\n",
    "            DETECTION_THRESHOLD = 0.3 #@param {type:\"number\"}\n",
    "            TEMP_FILE = img\n",
    "            FILE_NAME = TEMP_FILE.split(\"/\")[-1]\n",
    "            #print('test1.2 : ok')\n",
    "            #!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\n",
    "            #!wget -q -O $INPUT_IMAGE_URL\n",
    "            #print('test1.3 : ok')\n",
    "            try:\n",
    "                with open(TEMP_FILE) as file:\n",
    "                    #im = Image.open(TEMP_FILE)\n",
    "                    #im.thumbnail((512, 512), Image.ANTIALIAS)\n",
    "                    #im.save(TEMP_FILE, 'JPEG')\n",
    "                    # Run inference and draw detection result on the local copy of the original file\n",
    "                    detection_result_image, result = run_odt_and_draw_results(\n",
    "                        TEMP_FILE,\n",
    "                        interpreter,\n",
    "                        threshold=DETECTION_THRESHOLD\n",
    "                    )\n",
    "                    results.append(result)\n",
    "                    # Show the detection result\n",
    "                    #Image.fromarray(detection_result_image)\n",
    "                    #result = Image.fromarray(detection_result_image)\n",
    "                    #result.save(\"{}{}\".format(output_images_directory, FILE_NAME), 'JPEG')\n",
    "                    #result.show()\n",
    "            except OSError as e:\n",
    "                print(e.errno)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_prediction_test_dataset(batch_size):\n",
    "    results = []\n",
    "    list_of_images = get_test_images()\n",
    "    interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    #list_of_images = ['/home/acarlier/code/test/2021-06-11-13-44-43.jpg']\n",
    "    # Load the TFLite model\n",
    "    print(len(list_of_images),' to predict')\n",
    "    for i in range(0, len(list_of_images), batch_size):\n",
    "        \n",
    "        images = list_of_images[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        DETECTION_THRESHOLD = 0.3 #@param {type:\"number\"}\n",
    "        detection_result_image, result = run_odt_and_draw_results(\n",
    "            images,\n",
    "            interpreter,\n",
    "            threshold=DETECTION_THRESHOLD\n",
    "        )\n",
    "        results.append(result)\n",
    "\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3516  to predict\n",
      "0\n",
      "interpreter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2022-03-22 17:40:46.162610: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-22 17:40:48.369337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9977 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2022-03-22 17:40:48.372187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10406 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:a1:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "1\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "2\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "3\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "4\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "5\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "6\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "7\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "8\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "9\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "10\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "11\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "12\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "13\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "14\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "15\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "16\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "17\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "18\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "19\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "20\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "21\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "22\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "23\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "24\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "25\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "26\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "27\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "28\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "29\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "30\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "31\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "32\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "33\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "34\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "35\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "36\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "37\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "38\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "39\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "40\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "41\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "42\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "43\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "44\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "45\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "46\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "47\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "48\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "49\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "50\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "51\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "52\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "53\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "54\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "55\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "56\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "57\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "58\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "59\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "60\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "61\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "62\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "63\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "64\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "65\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "66\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "67\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "68\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "69\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "70\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "71\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "72\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "73\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "74\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "75\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "76\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "77\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "78\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "79\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "80\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "81\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "82\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "83\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "84\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "85\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "86\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "87\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n",
      "après modèle\n",
      "detection\n",
      "88\n",
      "interpreter\n",
      "tf.Tensor([  1 384 384   3], shape=(4,), dtype=int32)\n",
      "preprocessing\n",
      "avant modèle\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000009?line=0'>1</a>\u001b[0m results_predictions \u001b[39m=\u001b[39m prediction_test_dataset()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000009?line=1'>2</a>\u001b[0m \u001b[39m#lts_predictions = prediction_test_dataset()\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000009?line=2'>3</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(results_predictions, \u001b[39mopen\u001b[39m( \u001b[39m\"\u001b[39m\u001b[39m/home/acarlier/code/OrnithoScope/saved_models/results.p\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m ) )\n",
      "\u001b[1;32m/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb Cell 9'\u001b[0m in \u001b[0;36mprediction_test_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=25'>26</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(TEMP_FILE) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=27'>28</a>\u001b[0m         \u001b[39m#im = Image.open(TEMP_FILE)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=28'>29</a>\u001b[0m         \u001b[39m#im.thumbnail((512, 512), Image.ANTIALIAS)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=29'>30</a>\u001b[0m         \u001b[39m#im.save(TEMP_FILE, 'JPEG')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=30'>31</a>\u001b[0m         \u001b[39m# Run inference and draw detection result on the local copy of the original file\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=31'>32</a>\u001b[0m         detection_result_image, result \u001b[39m=\u001b[39m run_odt_and_draw_results(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=32'>33</a>\u001b[0m             TEMP_FILE,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=33'>34</a>\u001b[0m             interpreter,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=34'>35</a>\u001b[0m             threshold\u001b[39m=\u001b[39;49mDETECTION_THRESHOLD\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=35'>36</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=36'>37</a>\u001b[0m         results\u001b[39m.\u001b[39mappend(result)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=37'>38</a>\u001b[0m         \u001b[39m# Show the detection result\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=38'>39</a>\u001b[0m         \u001b[39m#Image.fromarray(detection_result_image)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=39'>40</a>\u001b[0m         \u001b[39m#result = Image.fromarray(detection_result_image)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=40'>41</a>\u001b[0m         \u001b[39m#result.save(\"{}{}\".format(output_images_directory, FILE_NAME), 'JPEG')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000008?line=41'>42</a>\u001b[0m         \u001b[39m#result.show()\u001b[39;00m\n",
      "\u001b[1;32m/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb Cell 6'\u001b[0m in \u001b[0;36mrun_odt_and_draw_results\u001b[0;34m(image_path, interpreter, threshold)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=88'>89</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpreprocessing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=90'>91</a>\u001b[0m \u001b[39m# Run object detection on the input image\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=91'>92</a>\u001b[0m results \u001b[39m=\u001b[39m detect_objects(interpreter, preprocessed_image, threshold\u001b[39m=\u001b[39;49mthreshold)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=92'>93</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdetection\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=94'>95</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=95'>96</a>\u001b[0m   \u001b[39m# Plot the detection results on the input image\u001b[39;00m\n",
      "\u001b[1;32m/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb Cell 6'\u001b[0m in \u001b[0;36mdetect_objects\u001b[0;34m(interpreter, image, threshold)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=52'>53</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mavant modèle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=53'>54</a>\u001b[0m \u001b[39m# Feed the input image to the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=54'>55</a>\u001b[0m output \u001b[39m=\u001b[39m signature_fn(images\u001b[39m=\u001b[39;49mimage)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=55'>56</a>\u001b[0m \u001b[39m#interpreter.set_tensor(0, image)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=56'>57</a>\u001b[0m \u001b[39m#interpreter.invoke()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/acarlier/code/OrnithoScope/notebooks/model_metrics/metrics_efficientDet1.ipynb#ch0000005?line=57'>58</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39maprès modèle\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py:258\u001b[0m, in \u001b[0;36mSignatureRunner.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/acarlier/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py?line=253'>254</a>\u001b[0m \u001b[39mfor\u001b[39;00m input_name, value \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    <a href='file:///home/acarlier/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py?line=254'>255</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpreter_wrapper\u001b[39m.\u001b[39mSetTensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inputs[input_name], value,\n\u001b[1;32m    <a href='file:///home/acarlier/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py?line=255'>256</a>\u001b[0m                                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_subgraph_index)\n\u001b[0;32m--> <a href='file:///home/acarlier/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py?line=257'>258</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter_wrapper\u001b[39m.\u001b[39;49mInvoke(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_subgraph_index)\n\u001b[1;32m    <a href='file:///home/acarlier/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py?line=258'>259</a>\u001b[0m result \u001b[39m=\u001b[39m {}\n\u001b[1;32m    <a href='file:///home/acarlier/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py?line=259'>260</a>\u001b[0m \u001b[39mfor\u001b[39;00m output_name, output_index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_predictions = prediction_test_dataset()\n",
    "#lts_predictions = prediction_test_dataset()\n",
    "pickle.dump(results_predictions, open( \"/home/acarlier/code/OrnithoScope/saved_models/results.p\", \"wb\" ) )\n",
    "#results_predictions = pickle.load( open( \"/home/acarlier/code/OrnithoScope/saved_models/results.p\", \"rb\" ) )\n",
    "classes = get_classes_from_input()\n",
    "print(len(results_predictions))\n",
    "print(classes)\n",
    "results_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation pour le calcul des precision recall\n",
    "Il faut convertir les id de label en nom de label - DONE </br>\n",
    "Il faut regrouper les label selon les file path pour avoir une liste de labels par file_path </br>\n",
    "Ensuite comparaison de la liste des labels prédits et la liste des labels de vérité terrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion de l'id en nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_id_to_label_name(list_of_results,list_of_classes):\n",
    "    results = copy.deepcopy(list_of_results)\n",
    "    for i in range(len(results)):\n",
    "        for result in results[i]:\n",
    "            id = int(result['class_id'])\n",
    "            result['class_id'] = list_of_classes[id]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_converted = convert_id_to_label_name(results_predictions, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MESCHA', 'SITTOR', 'MESBLE', 'MESNON', 'PINARB', 'ACCMOU', 'ROUGOR', 'VEREUR', 'MOIDOM', 'TOUTUR', 'ECUROU', 'PIEBAV', 'MULGRI', 'CAMPAG', 'MESNOI', 'MESHUP']\n",
      "Before  [[{'bounding_box': array([0.09528723, 0.25329775, 0.69151413, 0.4730904 ], dtype=float32), 'class_id': 7.0, 'score': 0.66796875}], [{'bounding_box': array([0.4092438, 0.5277908, 1.0296512, 0.7281122], dtype=float32), 'class_id': 0.0, 'score': 0.375}]]\n",
      "After  [[{'bounding_box': array([0.09528723, 0.25329775, 0.69151413, 0.4730904 ], dtype=float32), 'class_id': 'VEREUR', 'score': 0.66796875}], [{'bounding_box': array([0.4092438, 0.5277908, 1.0296512, 0.7281122], dtype=float32), 'class_id': 'MESCHA', 'score': 0.375}], [{'bounding_box': array([0.5509056, 0.5129066, 0.9888495, 0.9870934], dtype=float32), 'class_id': 'MESCHA', 'score': 0.9296875}], [{'bounding_box': array([0.39843464, 0.30333823, 0.6451436 , 0.5669474 ], dtype=float32), 'class_id': 'VEREUR', 'score': 0.84765625}]]\n"
     ]
    }
   ],
   "source": [
    "print(classes)\n",
    "print(\"Before \",results_predictions[:2])\n",
    "print(\"After \",results_converted[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_label_from_prediction(list_of_results):\n",
    "    results = copy.deepcopy(list_of_results)\n",
    "    l_final = []\n",
    "    for i in range(len(results)):\n",
    "        l = []\n",
    "        for result in results[i]:\n",
    "            label = result['class_id']\n",
    "            l.append(label)\n",
    "        l_final.append(l)\n",
    "    return l_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['VEREUR'], ['MESCHA'], ['MESCHA'], ['VEREUR'], [], ['MESCHA'], ['VEREUR', 'MESCHA'], ['MESCHA'], ['MESCHA'], ['MESCHA']]\n"
     ]
    }
   ],
   "source": [
    "list_of_label_from_prediction = get_list_of_label_from_prediction(results_converted)\n",
    "print(list_of_label_from_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupby sur le DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3771, 10)\n",
      "3516\n",
      "2124    /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090002_(13.0).jpg\n",
      "2125     /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090008_(9.0).jpg\n",
      "2126    /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090012_(23.0).jpg\n",
      "2127    /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090017_(10.0).jpg\n",
      "2128    /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090017_(10.0).jpg\n",
      "2129    /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090021_(14.0).jpg\n",
      "2130    /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090021_(14.0).jpg\n",
      "2131     /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090037_(6.0).jpg\n",
      "2132     /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090043_(7.0).jpg\n",
      "2133     /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090105_(5.0).jpg\n",
      "Name: file_path, dtype: object\n",
      "columns  Index(['split_value', 'file_path', 'label', 'x_min', 'y_min', 'empty_1',\n",
      "       'empty_2', 'x_max', 'y_max', 'empty_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_input = pd.read_csv('/home/acarlier/code/OrnithoScope/data/input.csv', names = columns_names)\n",
    "df_input_test = df_input[df_input[\"split_value\"] == 'TEST']\n",
    "print(df_input_test.shape)\n",
    "print(len(df_input_test['file_path'].unique()))\n",
    "print(df_input_test['file_path'].head(10))\n",
    "print('columns ',df_input_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_groupby_filepath(df_input_test):\n",
    "    df_test = copy.deepcopy(df_input_test)\n",
    "    df_test = df_test[['file_path','label']]\n",
    "    print(df_test.columns)\n",
    "    #df_test = df_test.groupby('file_path')\n",
    "    #df_test.groupby('file_path')['label'].apply(lambda x: \"[%s]\" % ', '.join(x))\n",
    "    df = df_test.groupby(['file_path'], sort=False)['label'].apply(lambda x: list(x)).to_frame().reset_index()\n",
    "    #\"%s\" % ', '.join(x)\n",
    "    print(df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['file_path', 'label'], dtype='object')\n",
      "Index(['file_path', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_groupedby = df_groupby_filepath(df_input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3516\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090002_(13.0).jpg</td>\n",
       "      <td>[VEREUR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090008_(9.0).jpg</td>\n",
       "      <td>[MESCHA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090012_(23.0).jpg</td>\n",
       "      <td>[MESCHA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090017_(10.0).jpg</td>\n",
       "      <td>[MESCHA, VEREUR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090021_(14.0).jpg</td>\n",
       "      <td>[MESCHA, MESCHA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090037_(6.0).jpg</td>\n",
       "      <td>[MESCHA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090043_(7.0).jpg</td>\n",
       "      <td>[VEREUR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090105_(5.0).jpg</td>\n",
       "      <td>[MESCHA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090115_(7.0).jpg</td>\n",
       "      <td>[MESCHA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090133_(8.0).jpg</td>\n",
       "      <td>[MESCHA]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                           file_path  \\\n",
       "0  /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090002_(13.0).jpg   \n",
       "1   /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090008_(9.0).jpg   \n",
       "2  /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090012_(23.0).jpg   \n",
       "3  /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090017_(10.0).jpg   \n",
       "4  /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090021_(14.0).jpg   \n",
       "5   /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090037_(6.0).jpg   \n",
       "6   /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090043_(7.0).jpg   \n",
       "7   /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090105_(5.0).jpg   \n",
       "8   /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090115_(7.0).jpg   \n",
       "9   /home/acarlier/OrnithoMate/p0133_bird_data/raw_data/task_2021-03-01_09/20210301-090133_(8.0).jpg   \n",
       "\n",
       "              label  \n",
       "0          [VEREUR]  \n",
       "1          [MESCHA]  \n",
       "2          [MESCHA]  \n",
       "3  [MESCHA, VEREUR]  \n",
       "4  [MESCHA, MESCHA]  \n",
       "5          [MESCHA]  \n",
       "6          [VEREUR]  \n",
       "7          [MESCHA]  \n",
       "8          [MESCHA]  \n",
       "9          [MESCHA]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_groupedby.shape[0])\n",
    "df_groupedby.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul des precisions recall par classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_by_comparing_two_lists(list1,list2):\n",
    "    # cas où les listes sont égales\n",
    "    metrics = {'TP':0, 'FP':0, 'FN':0}\n",
    "    if list1 == list2:\n",
    "        metrics['TP'] += len(list1)\n",
    "    else:\n",
    "        for element in list1:\n",
    "            if element in list2:\n",
    "                metrics['TP'] += 1\n",
    "            elif element not in list2:\n",
    "                metrics['FP'] += 1\n",
    "        for element in list2:\n",
    "            if element not in list1:\n",
    "                metrics['FN'] += 1\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiple_detections(true_labels, pred_labels):\n",
    "  TP = []\n",
    "  FP = pred_labels.copy()\n",
    "  FN = true_labels.copy()\n",
    "\n",
    "  for pl in pred_labels:\n",
    "    if pl in true_labels:\n",
    "      FP.remove(pl)\n",
    "      FN.remove(pl)\n",
    "      TP.append(pl)\n",
    "\n",
    "  return TP, FN, FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 element : \n",
      " (['V'], [], []) \n",
      " ([], ['V'], ['M']) \n",
      " ([], ['M'], ['V']) \n",
      " 2 elements : \n",
      " (['M', 'V'], [], []) \n",
      " (['M', 'V'], [], []) \n",
      " (['I'], ['M'], ['V'])\n"
     ]
    }
   ],
   "source": [
    "# première liste = prédiction et seconde liste = vérité terrain\n",
    "test0 = evaluate_multiple_detections(['V'],['V']) #TP = 1\n",
    "test1 = evaluate_multiple_detections(['V'],['M']) #FP = 1, FN = 1\n",
    "test2 = evaluate_multiple_detections(['M'],['V']) #FN = 1, FP = 1\n",
    "\n",
    "test0_2 = evaluate_multiple_detections(['M','V'],['M','V']) #TP = 2\n",
    "test1_2 = evaluate_multiple_detections(['V', 'M'],['M','V']) #TP = 2\n",
    "test2_2 = evaluate_multiple_detections(['M','I'],['V','I']) #TP = 1, FP = \n",
    "\n",
    "print(f\"\\n 1 element : \\n {test0} \\n {test1} \\n {test2} \\n 2 elements : \\n {test0_2} \\n {test1_2} \\n {test2_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall_from_prediction(list_of_results, df_input_test, list_of_classes):\n",
    "    class_metrics = []\n",
    "    for classes in list_of_classes:\n",
    "        class_metrics.append({'TP':0, 'FP':0, 'FN':0})\n",
    "    \n",
    "    for i in range(len(list_of_results)):\n",
    "        pred_labels = list_of_results[i]\n",
    "        true_labels = df_input_test.iloc[i]['label']\n",
    "        TP, FN, FP = evaluate_multiple_detections(true_labels, pred_labels)\n",
    "        print(TP, FN, FP)\n",
    "        \n",
    "        for lab in TP:\n",
    "            class_metrics[list_of_classes.index(lab)]['TP'] += 1\n",
    "        \n",
    "        for lab in FN:\n",
    "            class_metrics[list_of_classes.index(lab)]['FN'] += 1\n",
    "            \n",
    "        for lab in FP:\n",
    "            class_metrics[list_of_classes.index(lab)]['FP'] += 1    \n",
    "     \n",
    "    class_res = []  \n",
    "    for i in range(len(list_of_classes)):\n",
    "        P = class_metrics[i]['TP'] / max(class_metrics[i]['TP'] + class_metrics[i]['FP'], 1)\n",
    "        R = class_metrics[i]['TP'] / max(class_metrics[i]['TP'] + class_metrics[i]['FN'], 1)\n",
    "        if P == 0 and R == 0:\n",
    "            F_score = 0\n",
    "        else:\n",
    "            F_score = 2*P*R/(P+R)\n",
    "\n",
    "        class_res.append({'Precision': P, 'Rappel': R, 'F-score': F_score})\n",
    "\n",
    "    #accuracy = 100*total_correct_detections/(total_correct_detections + toal_incorrect_detections)\n",
    "    \n",
    "\n",
    "    return class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VEREUR'] [] []\n",
      "['MESCHA'] [] []\n",
      "['MESCHA'] [] []\n",
      "['VEREUR'] ['MESCHA'] []\n",
      "[] ['MESCHA', 'MESCHA'] []\n",
      "['MESCHA'] [] []\n",
      "['VEREUR'] [] ['MESCHA']\n",
      "['MESCHA'] [] []\n",
      "['MESCHA'] [] []\n",
      "['MESCHA'] [] []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'TP': 6, 'FP': 1, 'FN': 3},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 3, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0},\n",
       " {'TP': 0, 'FP': 0, 'FN': 0}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics = get_precision_recall_from_prediction(list_of_label_from_prediction, df_groupedby, classes)\n",
    "class_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(class_metrics, list_of_species):\n",
    "    for i in range(0, len(class_metrics)):\n",
    "        TP = class_metrics[i]['TP']\n",
    "        FP = class_metrics[i]['FP']\n",
    "        FN = class_metrics[i]['FN']\n",
    "        print(f\"{list_of_species[i]} - TP : {TP} FP : {FP} FN : {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESCHA - TP : 0 FP : 0 FN : 0\n",
      "SITTOR - TP : 0 FP : 0 FN : 0\n",
      "MESBLE - TP : 0 FP : 0 FN : 0\n",
      "MESNON - TP : 0 FP : 0 FN : 0\n",
      "PINARB - TP : 0 FP : 0 FN : 0\n",
      "ACCMOU - TP : 0 FP : 0 FN : 0\n",
      "ROUGOR - TP : 0 FP : 0 FN : 0\n",
      "VEREUR - TP : 0 FP : 0 FN : 0\n",
      "MOIDOM - TP : 0 FP : 0 FN : 0\n",
      "TOUTUR - TP : 0 FP : 0 FN : 0\n",
      "ECUROU - TP : 0 FP : 0 FN : 0\n",
      "PIEBAV - TP : 0 FP : 0 FN : 0\n",
      "MULGRI - TP : 0 FP : 0 FN : 0\n",
      "CAMPAG - TP : 0 FP : 0 FN : 0\n",
      "MESNOI - TP : 0 FP : 0 FN : 0\n",
      "MESHUP - TP : 0 FP : 0 FN : 0\n"
     ]
    }
   ],
   "source": [
    "print_results(class_metrics, get_classes_from_input())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19c3081fe3265570dc101441fe38429c8bca377ba2301426b88b33379fbbcf85"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ornithoScope_virtualenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
